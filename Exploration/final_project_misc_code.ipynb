{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn():\n",
    "    for i in range(1, 2):\n",
    "        knn = KNeighborsClassifier(n_neighbors=i)\n",
    "\n",
    "        knn.fit(train_data, train_labels) #train on processed train data\n",
    "\n",
    "        pred_knn  = knn.predict(dev_data) #predict on dev data\n",
    "\n",
    "        accuracy_count_knn = 0\n",
    "\n",
    "        for j in range(pred_knn.shape[0]):\n",
    "            if pred_knn[j] == dev_labels[j]:\n",
    "                accuracy_count_knn += 1\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        print(f'KNN accuracy at i={i} : {(accuracy_count_knn/pred_knn.shape[0])}')\n",
    "\n",
    "        if i == 1:\n",
    "            plot_confusion_matrix(dev_labels, pred_knn)\n",
    "    \n",
    "#     ########## just type 1 and 2 ##########\n",
    "    train_data_1_2 = train_data[(train_labels == 1) | (train_labels == 2)]\n",
    "    train_labels_1_2 = train_labels[(train_labels == 1) | (train_labels == 2)]\n",
    "    pred_knn_1_2_data = dev_data[(pred_knn == 1) | (pred_knn == 2)]\n",
    "    pred_knn_1_2_actual_labels = dev_labels[(pred_knn == 1) | (pred_knn == 2)]\n",
    "    \n",
    "    \n",
    "    baseline_models(train_data_1_2, train_labels_1_2, pred_knn_1_2_data, pred_knn_1_2_actual_labels)\n",
    "    \n",
    "      \n",
    "knn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bernoulli_nb(alphas):\n",
    "\n",
    "    # BernoulliNB training\n",
    "    model_nb = BernoulliNB()\n",
    "    model_nb.fit(train_data, train_labels)\n",
    "\n",
    "    # use cros validation = 5 to validate the alphas\n",
    "    model_cv = GridSearchCV(estimator=model_nb, param_grid=alphas, cv=5, scoring='accuracy')\n",
    "    model_cv.fit(train_data, train_labels)\n",
    "\n",
    "    # create the data frame\n",
    "    params = [param['alpha'] for param in model_cv.cv_results_[\"params\"]]\n",
    "    mean_test_score = model_cv.cv_results_[\"mean_test_score\"]\n",
    "    std_test_score = model_cv.cv_results_[\"std_test_score\"]\n",
    "    df = pd.DataFrame(list(zip(params, mean_test_score,std_test_score)),\n",
    "                   columns =['alpha', 'mean_test_score',\"std_test_score\"])\n",
    "    print(df)\n",
    "    \n",
    "alphas = {'alpha': [1.0e-10, 0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 10.0]}\n",
    "bernoulli_nb(alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_nb():\n",
    "\n",
    "\n",
    "    ####Train on GaussianNB with no var_smoothing####\n",
    "    model_gnb = GaussianNB()\n",
    "    model_gnb.fit(train_data,train_labels)\n",
    "\n",
    "    sigmas = model_gnb.sigma_\n",
    "    thetas = model_gnb.theta_\n",
    "\n",
    "    ## predict on dev data\n",
    "\n",
    "    pred_gnb = model_gnb.predict(dev_data)\n",
    "    accuracy_count_gnb = 0\n",
    "\n",
    "\n",
    "    ###generate accuracy\n",
    "    for j in range(dev_labels.shape[0]):\n",
    "        if pred_gnb[j] == dev_labels[j]:\n",
    "            accuracy_count_gnb += 1\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    print(\"GaussianNB accuracy w/ no modification: \" + str(accuracy_count_gnb/pred_gnb.shape[0]))\n",
    "\n",
    "    ###Train on gaussianNB with var_smoothing = .1###\n",
    "\n",
    "    model_gnb_s = GaussianNB(var_smoothing=.1)\n",
    "    model_gnb_s.fit(train_data,train_labels)\n",
    "\n",
    "    sigmas2 = model_gnb_s.sigma_\n",
    "    thetas2 = model_gnb_s.theta_\n",
    "\n",
    "\n",
    "    ## predict on dev data\n",
    "\n",
    "    pred_gnb_s = model_gnb_s.predict(dev_data)\n",
    "    accuracy_count_gnb_s = 0\n",
    "\n",
    "\n",
    "    ###generate accuracy\n",
    "    for j in range(dev_labels.shape[0]):\n",
    "        if pred_gnb_s[j] == dev_labels[j]:\n",
    "            accuracy_count_gnb_s += 1\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    print(\"GaussianNB accuracy w/ var smoothing @ .1: \" + str(accuracy_count_gnb_s/pred_gnb_s.shape[0]))\n",
    "\n",
    "    #train on bernoulliNB\n",
    "\n",
    "    model_gnb_s2 = BernoulliNB()\n",
    "    model_gnb_s2.fit(train_data,train_labels)\n",
    "\n",
    "    pred_gnb_s2 = model_gnb_s2.predict(dev_data)\n",
    "    accuracy_count_gnb_s2 = 0\n",
    "\n",
    "    for j in range(dev_labels.shape[0]):\n",
    "        if pred_gnb_s2[j] == dev_labels[j]:\n",
    "            accuracy_count_gnb_s2 += 1\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    print(\"Bernoulli NB performance: \" + str(accuracy_count_gnb_s2/pred_gnb_s2.shape[0]))\n",
    "\n",
    "\n",
    "    \n",
    "gaussian_nb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "# logistic regression\n",
    "\n",
    "logistic_result = {}\n",
    "logistic_result_weights = {}\n",
    "c_values = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1]\n",
    "\n",
    "for c in c_values:\n",
    "    model_logistic = LogisticRegression(C = c, solver=\"liblinear\", multi_class=\"auto\")\n",
    "    model_logistic.fit(train_data, train_labels)\n",
    "    model_logistic_pred = model_logistic.predict(dev_data)\n",
    "    logistic_result[c] = metrics.accuracy_score(dev_labels, model_logistic_pred)\n",
    "\n",
    "# find best c\n",
    "best_c = max(logistic_result, key=logistic_result.get)\n",
    "\n",
    "# build dataframe of f1_score at different c values\n",
    "logistic_result_df = pd.DataFrame(data = zip(c_values,list(logistic_result.values())), columns = ['c','accuracy'])\n",
    "logistic_result_df['regularization'] = 1 / np.array(c_values)\n",
    "\n",
    "# print results\n",
    "print('Logistic Regression Classifier:')\n",
    "print('-'*50)\n",
    "print(f'Best \"c\" in logistic regression is: c={best_c}, F1 score={round(logistic_result[best_c],3)}')\n",
    "print()\n",
    "print(logistic_result_df[['c','regularization','accuracy']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest(train_data=train_data, \n",
    "    train_labels=train_labels, \n",
    "    dev_data=dev_data, \n",
    "    dev_labels=dev_labels,\n",
    "    show_confusion_matrix=True):\n",
    "    # Random Forest\n",
    "    rf = RandomForestClassifier(random_state=0)\n",
    "    rf.fit(train_data, train_labels)\n",
    "    print(f'\\nRandom Forest accuracy = {(rf.score(dev_data, dev_labels)*100)}')\n",
    "    pred_labels  = rf.predict(dev_data)\n",
    "    if show_confusion_matrix == True:\n",
    "        plot_confusion_matrix(dev_labels, pred_labels)\n",
    "        \n",
    "    print(f'cover type 1 and type 2 total correct {np.sum(np.diag(metrics.confusion_matrix(dev_labels, pred_labels))[:2])}')\n",
    "\n",
    "random_forest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_soil_type_features(dataset):\n",
    "    # get feature indexes for Soil_Types\n",
    "    other_idx = [i for i, col in enumerate(train.columns) if not 'Soil' in col and 'Cover' not in col]\n",
    "    soil_type_idx = [i for i, col in enumerate(train.columns) if 'Soil' in col]\n",
    "    \n",
    "    soil_type_features = dataset[:, soil_type_idx]\n",
    "    other_features = dataset[:, other_idx]\n",
    "    \n",
    "    return other_features, soil_type_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spca_n_comp_trials(split_features=True):\n",
    "    print('without spca:')\n",
    "    baseline_models(\n",
    "            train_data=train_data, \n",
    "            train_labels=train_labels, \n",
    "            dev_data=dev_data,\n",
    "            dev_labels=dev_labels,\n",
    "            show_confusion_matrix=False\n",
    "        )\n",
    "    \n",
    "    for alpha_set in [1,10,100]:\n",
    "        for n_comp in range(1,6):\n",
    "            print()\n",
    "            print('-'*25)\n",
    "            print(f'number of components = {n_comp}. alpha = {alpha_set}')\n",
    "\n",
    "            \n",
    "            if split_features == True:\n",
    "                ##### transform train_data #####\n",
    "                other_features, soil_type_features = split_soil_type_features(train_data)\n",
    "                spca = SparsePCA(n_components = n_comp, alpha=alpha_set, tol=0.1) # always done now, depreciated #, normalize_components=True)\n",
    "                spca_soil_features = spca.fit_transform(soil_type_features)\n",
    "                spca_transformed_train_data = np.concatenate((other_features, spca_soil_features), axis=1)\n",
    "\n",
    "                ##### transform dev_data #####\n",
    "                other_features, soil_type_features = split_soil_type_features(dev_data)\n",
    "                spca = SparsePCA(n_components = n_comp, alpha=alpha_set, tol=0.1) # always done now, depreciated #, normalize_components=True)\n",
    "                spca_soil_features = spca.fit_transform(soil_type_features)\n",
    "                spca_transformed_dev_data = np.concatenate((other_features, spca_soil_features), axis=1)\n",
    "            else:\n",
    "                ##### transform train_data #####\n",
    "                spca = SparsePCA(n_components = n_comp, alpha=alpha_set, tol=0.1) # always done now, depreciated #, normalize_components=True)\n",
    "                spca_transformed_train_data = spca.fit_transform(train_data)\n",
    "\n",
    "                ##### transform dev_data #####\n",
    "                spca = SparsePCA(n_components = n_comp, alpha=alpha_set, tol=0.1) # always done now, depreciated #, normalize_components=True)\n",
    "                spca_transformed_dev_data = spca.fit_transform(dev_data)\n",
    "\n",
    "            baseline_models(\n",
    "                train_data=spca_transformed_train_data, \n",
    "                train_labels=train_labels, \n",
    "                dev_data=spca_transformed_dev_data,\n",
    "                dev_labels=dev_labels,\n",
    "                show_confusion_matrix=False\n",
    "            )\n",
    "    return\n",
    "\n",
    "spca_n_comp_trials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spca_n_comp_trials(split_features=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_n_comp_trials(split_features=True):\n",
    "    print('without pca:')\n",
    "    baseline_models(\n",
    "            train_data=train_data, \n",
    "            train_labels=train_labels, \n",
    "            dev_data=dev_data,\n",
    "            dev_labels=dev_labels,\n",
    "            show_confusion_matrix=False\n",
    "        )\n",
    "    \n",
    "    for n_comp in range(1,6):\n",
    "        print()\n",
    "        print('-'*25)\n",
    "        print(f'number of components = {n_comp}')\n",
    "\n",
    "        if split_features == True:\n",
    "            ##### transform train_data #####\n",
    "            other_features, soil_type_features = split_soil_type_features(train_data)\n",
    "            pca = PCA(n_components = n_comp) # always done now, depreciated #, normalize_components=True)\n",
    "            pca_soil_features = pca.fit_transform(soil_type_features)\n",
    "            pca_transformed_train_data = np.concatenate((other_features, pca_soil_features), axis=1)\n",
    "\n",
    "            ##### transform dev_data #####\n",
    "            other_features, soil_type_features = split_soil_type_features(dev_data)\n",
    "            pca = PCA(n_components = n_comp) # always done now, depreciated #, normalize_components=True)\n",
    "            pca_soil_features = pca.fit_transform(soil_type_features)\n",
    "            pca_transformed_dev_data = np.concatenate((other_features, pca_soil_features), axis=1)\n",
    "        else:\n",
    "            ##### transform train_data #####\n",
    "            pca = PCA(n_components = n_comp) # always done now, depreciated #, normalize_components=True)\n",
    "            pca_transformed_train_data = pca.fit_transform(train_data)\n",
    "\n",
    "            ##### transform dev_data #####\n",
    "            pca = PCA(n_components = n_comp) # always done now, depreciated #, normalize_components=True)\n",
    "            pca_transformed_dev_data = pca.fit_transform(dev_data)\n",
    "\n",
    "        baseline_models(\n",
    "            train_data=pca_transformed_train_data, \n",
    "            train_labels=train_labels, \n",
    "            dev_data=pca_transformed_dev_data,\n",
    "            dev_labels=dev_labels,\n",
    "            show_confusion_matrix=False\n",
    "        )\n",
    "    return\n",
    "\n",
    "pca_n_comp_trials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_n_comp_trials(split_features=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get feature indexes for Soil_Types\n",
    "other_idx = [i for i, col in enumerate(train.columns) if not 'Soil' in col and 'Cover' not in col]\n",
    "soil_type_idx = [i for i, col in enumerate(train.columns) if 'Soil' in col]\n",
    "\n",
    "soil_type_features = train_data[:, soil_type_idx]\n",
    "other_features = train_data[:, other_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a handy function for vizualizing principle components as a heatmap\n",
    "# this allows you to see what dimensions in the 'original space' are\n",
    "# active\n",
    "\n",
    "def visualize_pca_comps_heatmap(plot, comps, alpha=None):\n",
    "    heatmap = plot.pcolor(comps, cmap=plt.cm.Blues)\n",
    "    \n",
    "\n",
    "    x_lab = [i for i in range(comps.shape[1])]\n",
    "    y_lab = [i for i in (range(comps.shape[0]))]\n",
    "    \n",
    "    plot.set_xticks(np.arange(comps.shape[1])+0.5, minor=False)\n",
    "    plot.set_yticks(np.arange(comps.shape[0])+0.5, minor=False)\n",
    "    \n",
    "    # want a more natural, table-like display\n",
    "    plot.invert_yaxis()\n",
    "    \n",
    "    plot.set_xticklabels(x_lab, minor=False)\n",
    "    plot.set_yticklabels(y_lab, minor=False)\n",
    "    \n",
    "    if alpha is not None:\n",
    "        plt.title(f'Heatmap of Sparse PCA (alpha {alpha}) components Rows: components, Cols: Original dimensions. ')\n",
    "    else:\n",
    "        plt.title('Heatmap of PCA components Rows: components, Cols: Original dimensions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_testing():\n",
    "    # change alpha and watch as dimensions get dropped\n",
    "    n_comp = 2\n",
    "    \n",
    "    ###############\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    alpha_set = 0.1\n",
    "    spca = SparsePCA(n_components = n_comp, alpha=alpha_set, tol=0.1) # always done now, depreciated #, normalize_components=True)\n",
    "    spca.fit(soil_type_features)\n",
    "    p2 = plt.subplot(1, 1, 1)\n",
    "    visualize_pca_comps_heatmap(p2, spca.components_, alpha_set)\n",
    "    ###############\n",
    "\n",
    "    ###############\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    alpha_set = 1\n",
    "    spca = SparsePCA(n_components = n_comp, alpha=alpha_set, tol=0.1) # always done now, depreciated #, normalize_components=True)\n",
    "    spca.fit(soil_type_features)\n",
    "    p2 = plt.subplot(1, 1, 1)\n",
    "    visualize_pca_comps_heatmap(p2, spca.components_, alpha_set)\n",
    "    ###############\n",
    "\n",
    "    ###############\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    alpha_set = 10\n",
    "    spca = SparsePCA(n_components = n_comp, alpha=alpha_set, tol=0.1) # always done now, depreciated #, normalize_components=True)\n",
    "    spca.fit(soil_type_features)\n",
    "    p2 = plt.subplot(1, 1, 1)\n",
    "    visualize_pca_comps_heatmap(p2, spca.components_, alpha_set)\n",
    "    ###############\n",
    "\n",
    "    ###############\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    alpha_set = 100\n",
    "    spca = SparsePCA(n_components = n_comp, alpha=alpha_set, tol=0.1) # always done now, depreciated #, normalize_components=True)\n",
    "    spca.fit(soil_type_features)\n",
    "    p2 = plt.subplot(1, 1, 1)\n",
    "    visualize_pca_comps_heatmap(p2, spca.components_, alpha_set)\n",
    "    ###############\n",
    "\n",
    "pca_testing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_probability(predictions, probabilities):\n",
    "\n",
    "    pred_prob = np.array([])\n",
    "\n",
    "    for idx, value in enumerate(predictions):\n",
    "        if value == 1:\n",
    "            pred_prob = np.append(arr=pred_prob, values = probabilities[idx][0])\n",
    "        elif value == 2:\n",
    "            pred_prob = np.append(arr=pred_prob, values = probabilities[idx][1])\n",
    "        elif value == 3:\n",
    "            pred_prob = np.append(arr=pred_prob, values = probabilities[idx][2])\n",
    "        elif value == 4:\n",
    "            pred_prob = np.append(arr=pred_prob, values = probabilities[idx][3])\n",
    "        elif value == 5:\n",
    "            pred_prob = np.append(arr=pred_prob, values = probabilities[idx][4])\n",
    "        elif value == 6:\n",
    "            pred_prob = np.append(arr=pred_prob, values = probabilities[idx][5])\n",
    "        else:\n",
    "            pred_prob = np.append(arr=pred_prob, values = probabilities[idx][6])\n",
    "\n",
    "    return pred_prob\n",
    "\n",
    "#def ensemble_stage_model():\n",
    "\n",
    "train_data_norm = normalize_values(train_data, scaler_type = 'StandardScaler')\n",
    "dev_data_norm = normalize_values(dev_data, scaler_type = 'StandardScaler')\n",
    "\n",
    "\n",
    "model_1 = RandomForestClassifier(\n",
    "            criterion = 'entropy',\n",
    "            max_depth = None,\n",
    "            max_features = 'auto',\n",
    "            n_estimators= 150,\n",
    "            random_state=0\n",
    "        )\n",
    "model_2 = RandomForestClassifier(\n",
    "            criterion = 'entropy',\n",
    "            max_depth = None,\n",
    "            max_features = 'auto',\n",
    "            n_estimators= 150,\n",
    "            random_state=0\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "model_1.fit(train_data_norm, train_labels)\n",
    "model_2.fit(train_data, train_labels)\n",
    "\n",
    "\n",
    "model_1_pred = model_1.predict(dev_data_norm)\n",
    "model_2_pred = model_2.predict(dev_data)\n",
    "\n",
    "\n",
    "model_1_prob = model_1.predict_proba(dev_data_norm)\n",
    "model_2_prob = model_2.predict_proba(dev_data)\n",
    "\n",
    "\n",
    "model_1_pred_prob = prediction_probability(predictions=model_1_pred, probabilities=model_1_prob)\n",
    "model_2_pred_prob = prediction_probability(predictions=model_2_pred, probabilities=model_2_prob)\n",
    "\n",
    "\n",
    "ensemble_pred = np.array([])\n",
    "\n",
    "for idx in range(len(model_1_pred_prob)):\n",
    "    \n",
    "    prob_1 = model_1_pred_prob[idx]\n",
    "    prob_2 = model_2_pred_prob[idx]\n",
    "    \n",
    "    if prob_1 > prob_1:\n",
    "        pred = model_1_pred[idx]\n",
    "    else:\n",
    "        pred = model_2_pred[idx]\n",
    "        \n",
    "    ensemble_pred = np.append(arr=ensemble_pred, values = pred)\n",
    "\n",
    "\n",
    "model_1_accuracy = (metrics.accuracy_score(dev_labels, model_1_pred)*100)\n",
    "print(f'One Model accuracy: {model_1_accuracy}')\n",
    "model_2_stage_accuracy = (metrics.accuracy_score(dev_labels, ensemble_pred)*100)\n",
    "print(f'Ensemble accuracy: {model_2_stage_accuracy}')\n",
    "\n",
    "print('\\One Model Confusion Matrix')\n",
    "plot_confusion_matrix(dev_labels, model_1_pred)\n",
    "print('\\nEnsemble Confusion Matrix')\n",
    "plot_confusion_matrix(dev_labels, ensemble_pred)\n",
    "\n",
    "#ensemble_stage_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_probability(subset, base_class, predictions, probabilities):\n",
    "\n",
    "    pred_prob = np.array([])\n",
    "\n",
    "    if subset:\n",
    "    \n",
    "        for idx, value in enumerate(predictions):\n",
    "            if value == base_class:\n",
    "                pred_prob = np.append(arr=pred_prob, values = probabilities[idx][0])\n",
    "            else:\n",
    "                pred_prob = np.append(arr=pred_prob, values = probabilities[idx][1])\n",
    "                \n",
    "    else:\n",
    "        \n",
    "        for idx, value in enumerate(predictions):\n",
    "            if value == 1:\n",
    "                pred_prob = np.append(arr=pred_prob, values = probabilities[idx][0])\n",
    "            elif value == 2:\n",
    "                pred_prob = np.append(arr=pred_prob, values = probabilities[idx][1])\n",
    "            elif value == 3:\n",
    "                pred_prob = np.append(arr=pred_prob, values = probabilities[idx][2])\n",
    "            elif value == 4:\n",
    "                pred_prob = np.append(arr=pred_prob, values = probabilities[idx][3])\n",
    "            elif value == 5:\n",
    "                pred_prob = np.append(arr=pred_prob, values = probabilities[idx][4])\n",
    "            elif value == 6:\n",
    "                pred_prob = np.append(arr=pred_prob, values = probabilities[idx][5])\n",
    "            else:\n",
    "                pred_prob = np.append(arr=pred_prob, values = probabilities[idx][6])\n",
    "\n",
    "    return pred_prob\n",
    "\n",
    "#def ensemble_stage_model():\n",
    "\n",
    "train_data_norm = normalize_values(train_data, scaler_type = 'StandardScaler')\n",
    "dev_data_norm = normalize_values(dev_data, scaler_type = 'StandardScaler')\n",
    "\n",
    "\n",
    "# stage 1\n",
    "model_1 = RandomForestClassifier(\n",
    "            criterion = 'entropy',\n",
    "            max_depth = None,\n",
    "            max_features = 'auto',\n",
    "            n_estimators= 150,\n",
    "            random_state=0\n",
    "        )\n",
    "model_1.fit(train_data_norm, train_labels)\n",
    "model_1_pred = model_1.predict(dev_data_norm)\n",
    "model_1_prob = model_1.predict_proba(dev_data_norm)\n",
    "model_1_pred_prob = prediction_probability(subset=False, base_class=0, predictions=model_1_pred, probabilities=model_1_prob)\n",
    "\n",
    "# stage 2\n",
    "subset_condition = [1,2]\n",
    "train_data_subset, train_labels_subset, dev_data_subset, dev_labels_subset = get_subset(classes=subset_condition,\n",
    "                                                                                        train_data=train_data,\n",
    "                                                                                        dev_data=dev_data)\n",
    "model_2 = RandomForestClassifier(\n",
    "            criterion = 'gini',\n",
    "            max_depth = None,\n",
    "            max_features = 'auto',\n",
    "            n_estimators= 50,\n",
    "            random_state=0\n",
    "        )\n",
    "model_2.fit(train_data_subset, train_labels_subset)\n",
    "model_2_pred = model_2.predict(dev_data)\n",
    "model_2_prob = model_2.predict_proba(dev_data)\n",
    "model_2_pred_prob = prediction_probability(subset=True,base_class=1, predictions=model_2_pred, probabilities=model_2_prob)\n",
    "\n",
    "# stage 3\n",
    "subset_condition = [3,6]\n",
    "train_data_subset, train_labels_subset, dev_data_subset, dev_labels_subset = get_subset(classes=subset_condition,\n",
    "                                                                                        train_data=train_data,\n",
    "                                                                                        dev_data=dev_data)\n",
    "model_3 = RandomForestClassifier(\n",
    "            criterion = 'gini',\n",
    "            max_depth = None,\n",
    "            max_features = 'auto',\n",
    "            n_estimators= 150,\n",
    "            random_state=0\n",
    "        )\n",
    "model_3.fit(train_data_subset, train_labels_subset)\n",
    "model_3_pred = model_3.predict(dev_data)\n",
    "model_3_prob = model_3.predict_proba(dev_data)\n",
    "model_3_pred_prob = prediction_probability(subset=True, base_class=3, predictions=model_3_pred, probabilities=model_3_prob)\n",
    "\n",
    "# final prediction \n",
    "## if stage 1 predicted [1,2] then use stage 2 prediction\n",
    "## if stage 1 predicted [3,6] then use stage 3 prediction\n",
    "#model_2_stage_pred = np.array([])\n",
    "#model_3_stage_pred = np.array([])\n",
    "\n",
    "final_pred = np.array([])\n",
    "\n",
    "for idx in range(len(model_1_pred_prob)):\n",
    "    \n",
    "    prob_1 = float(model_1_pred_prob[idx])\n",
    "    prob_2 = float(model_2_pred_prob[idx])\n",
    "    prob_3 = float(model_3_pred_prob[idx])\n",
    "    \n",
    "    if (prob_2 > prob_1) and (prob_2 > prob_3):\n",
    "        pred = model_2_pred[idx]\n",
    "    elif prob_3 > prob_1 and prob_3 > prob_2:\n",
    "        pred = model_3_pred[idx]\n",
    "    else:\n",
    "        pred = model_1_pred[idx]\n",
    "        \n",
    "    final_pred = np.append(arr=final_pred, values = pred)\n",
    "\n",
    "\n",
    "model_1_accuracy = (metrics.accuracy_score(dev_labels, model_1_pred)*100)\n",
    "print(f'One stage accuracy: {model_1_accuracy}')\n",
    "model_2_stage_accuracy = (metrics.accuracy_score(dev_labels, final_pred)*100)\n",
    "print(f'One stage accuracy: {model_2_stage_accuracy}')\n",
    "\n",
    "\n",
    "print('\\nOne Stage Ensemble Confusion Matrix')\n",
    "plot_confusion_matrix(dev_labels, model_1_pred)\n",
    "print('\\nTwo Stage Ensemble Confusion Matrix')\n",
    "plot_confusion_matrix(dev_labels, final_pred)\n",
    "\n",
    "#ensemble_stage_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
